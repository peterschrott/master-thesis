# Framework design for automated feature extraction from semi-structured information entities

Structuring information, providing schemas and data normalization are solid fundamentals of data processing for about 5 decades now. Those well-known techniques have the common idea of giving a bag of unstructured information a unified schema. The ideas are based of several reasons, among which are the machine readability of information, simpler administration of data as well as the minimization of storage consumption. Major database systems like Oracle Database or MySQL have strong restrictions to meet the named requirements of structured data.


Groundbreaking new technologies rose over the last 10 years. Concepts like MapReduce loosened the strong necessities of pre-defined schemas and proper normalization. The technical foundation of the movement was the ability of usage of commodity hardware as well as the decreasing price of volatile memory. Document orientated and schema-less databases management systems were created due to similar reasons and supported the trend against unifying data structures. Full text search engines make use of inverted indices to make fast and successful data retrieval in messy data possible. The new mentality brings increased flexibility regarding data management and seems to form and settle.


Every blessing brings a curse. Equally does the contemporary style of handling and storing information. Schemas and normalization add sometimes very valuable insights, so called meta information, to the data. Among those are named attributes, definitions of data types or constrains. Various applications of data are easier to realize when meta data are available or work by far more precise when structural information are provided. These include, for example, information retrieval systems such as filter-engines, search-engines or recommendation systems as well as techniques in the field of machine learning, especially when it comes to feature selection and feature engineering. Well prepared data are also important when it comes to the application of mathematical functions on numbers. A further, equally important usage aspect is the exposure to end users, for example in web shops or desktop applications. All those applications utilize second level information in a different, indispensable fashion.


But neither data from normalized sources, and even less data from schema-less sources often fulfill the required granularity and purity of data for above mentioned use cases. Not only the underlying storage systems are to blame also the gathering of the data plays a significant role. Databases grow over time and might be poorly normalized; Information entities are often imported from diverse sources; Schemas vary, are underspecified or are interpreted differently. Lots of data mining and data cleaning steps are often involved to achieve the desired quality. A significant amount of manual work and know-how must be introduced to apply the latter methods appropriately.


This work is about to find a framework design which designates its abilities to tackle the above mentioned short comings of available data as well as to take full advantage of the dataâ€™s potential. Several approaches are discussed and examined to find a possible solution that automates the complex process of preparing semi-structured data in a way, such that it reaches a sufficient level of quality. Database systems, ontology, statistical learning theory or natural language processing are research fields in computer science providing highly promising methods to be considered as potential solution or as part of an ensemble that meets the stated requirements and therefore be analyzed for their fit.


Based on the researches a technical concept of the suggested framework and a proof of concept is developed within this work. It is showcased and evaluated in the domain of food recipes and their ingredients. Here, the food recipe is considered as information entity that comes in various semi-structured forms containing a set of ingredients. The ingredients itself are combinations of a defined set of attributes that have a multitude of manifestations as well as non-deterministic order. Other possible applications for the advice framework span from product catalogues to item databases. A toy dataset is created to demonstrate the performance of the proposed framework. A conclusion will assess the feasibility of such a system, review the performance result as well as address shortcomings.