\chapter{Fundamentals and related Technologies\label{cha:chapter2}}

\section{Fundamentals in data unification\label{sec:unification}}

Electronic data processing underlies applied mathematics and relies therefor on data of a certain, deterministic structure. In mathematics and computer science such rules of data representation are called canonical or normal form. Canonicalization is the discipline of transforming data of possible different forms of representation into a standardized form. An illustrative example of canonicalization is the representation of a boolean variable described in the XML schema type definition\cite{xml_schema_2017}. By definition a boolean variable supports binary logic representation, meaning that some state or flag can either be true or false. The set of possible literals might contain: 1, 0, true and false. The XML schema type definition specifies the canonical form of boolean as true and false, whereas 1 can be mapped to true and respectively 0 to false. Data in a normalized form can be transfered in any other form.
\\\\
The foundation of the idea of storing large data in a reusable format were captured by Edgar F. Codd in 1970 with his proposal for database normalization\cite{codd_1970}. The concept behind the proposal is to break the information down into entities with attributes and describe relations among them. A second and third version of database normalization followed and proposed a higher granularity and introduced relations between entities. The technique of data normalization is state of the art in rational database systems. Such a system stores data in a table format, where rows correspond to entities and columns to their attributes. We consider the entities \textit{person} and \textit{address} as an example. The \textit{person} entity holds the attributes \textit{surname} and \textit{name}, the \textit{address} entity the attributes \textit{street}, \textit{zip code} and \textit{city}. In addition the \textit{person} entity has a reference to the \textit{address} entity, thus a an \textit{address} can be related to a person. This relation is indicated by a unique key. Compare figure \ref{fig:rm}. Rational database systems enforces the entity definition, i.e. its schema and the predefined data types, and the relationship constraints strictly. Relationship modeling plays also a fundamental role in other data base and data management systems but are often not enforced on system level\cite{hills_2016}.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{rm.pdf}\\
  \caption{Entity model for person and address}\label{fig:rm}
\end{figure}

In contrast to relational databases, normalization is not mandatory in non rational, also call no SQL databases. This kind of databases are mostly used for web applications and in the field of big data. The same applies to entity relationships as the core concept of no SQL databases aims for de-normalized data storage. Considering the person-address example from above, all attributes of a person entity are stored with it, including its address. No SQL databases exist with a schema-leas approach, meaning there is no predefinition of attributes and corresponding data types. This also enables changes of the schema over time in the cardinality of attributes as well as their type. In the person-address example this means that the address is a non mandatory field and the zip code can be stored as number or string. Document-oriented databases, key-value storages and graph databases are the three most common no SQL databases.  Limitations are only set by the user's definition and imagination. This flexibility enormously supports the initially claimed chaotic data storage, though it is often limited by schema definitions on domain level. 
\\\\
Schemas help to maintain a understandable structure of data and underly a certain encoding format. They add determinism to data and make them less error prone. Schemas vary in their strictness, also encoding formats allow different levels of precision. CSV, JSON, XML and Protocol Buffers are a selection of the most commonly used formats for data interchange. From a top down view, an entity is defined by its attributes. When nesting is supported an entity can contain entities as attributes. In addition it is possible to define whether a field is mandatory or not. On attribute level the most common specification is the data type. The information if a field can be unset, i.e. nullable, can be provided too. Data types are usually not unique through different formats, but mappings usually exist. For a higher quality of data the individual type of an attribute can be restricted through ranges or enumerations, e.g. the value of field score can only accept integers from 1 to 5. Furthermore it is to distinguish between externally defined and self-contained schemas. External definition or external enforcement is the approach that drives rational databases and schema-full data storages. Is the schema woven with the actual data, we can speak about self-contained schemas. Schema-less storages benefit from self-contained schemas. Using such, data integrity is for the application level to maintain. As reference and further reading the book Database Schema Evolution and Meta-Modeling\cite{balsters_brock_conrad_2006} provides deeper insights.
\\\\
Summarizing we can say that the canonical form standardizes data representation, whereas normalization and schema definition add some additional information to it. A defined set of attributes as well as the respective types and value ranges can be considered as meta-data and make the data therefor more understandable by human as well as by machines. 

\section{Data storage technologies and data formats\label{sec:storage}}

In data driven applications the data are commonly fetched from a source. Sources in this circumstances are wide-ranging. A subset to be mentioned here are:
\begin{itemize}
  \item Purely file based
  \item Database systems
  \item REST interfaces
  \item Web sockets
  \item Message queues\\
\end{itemize}

\noindent\textbf{Purely file based} sources have almost no restrictions regarding the information format. The format is often defined on domain level. Human readable formats like CSV, JSON, XML or custom formats are some of the most common data structures. Additionally binary formats exists but are more often used in closed systems. Java serializable objects\cite{serializable_objects_2017}, protocol buffers\cite{protocol_buffers_2017} or BSON\cite{bson_2017} are some examples for binary formats.
\\\\
\textbf{Database systems} provide more boundaries when it comes to the data format. Database management system can be divided in schema-oriented and schema-less systems. The former, by definition, enforces a certain schema for individual information entities. In rational database systems the table schema must be predefined and is enforced by the DBMS. Also in other systems like key-value stores the encoding format of the value can be enforced. Schema-less systems, often document-oriented, bring more flexibility. The encoding format is defined per type of information entities. The actual schema, e.g. definition of attributes, is not enforced in such systems.
\\\\
\textbf{REST interfaces} deliver data in most cases in human readable encoding format\cite{rest_2017}. Some exception exist that expose binary information. The so called media type, also MIME type or content type is officially standardized by the Internet Assigned Numbers Authority (IANA)\cite{media_types_2017}. Dedicated content delivery APIs mostly utilize JSON or XML. For memory reduction zipped data are shipped. 
\\\\
\textbf{Web sockets} are originally designed for the communication between browsers and web servers and operate on a lower network layer, TCP, as REST interfaces for data exchange\cite{websocket_2017}. Projects exists where web sockets are used for data transfer and enables a streaming unlike REST, which works on a batch basis. Regarding formats, web sockets apply similarly loose restrictions as file based data sources does.
\\\\
\textbf{Message queues} are old techniques but embrace over the last years enormously. The trends shifts towards data streaming and directly process them rather to store data over the long-term. Often used for connecting systems, message queues do mostly not enforce a specific data formats also similar to file based sources.

\section{State analysis of heterogeneous data storage\label{sec:stateanalysis}}

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{quality-problems.pdf}\\
  \caption{Categorization of data quality problems}\label{fig:qualityproblems}
\end{figure}

In this section digitally available data are analyzed with respect to their quality as well as considering their source. Figure \ref{fig:qualityproblems} illustrates a categorization of the data quality issues into four sub problems. The breakdown shows that the devision correlates with the above stated classification of data storage principles. Therefor a general distinction between single and multi source problems is possible. In both categories a distinction on entity level can be made. As a reference compare the literature\cite{rahm_hai_do_2017}.

\subsection{Single source problems}

Single source problems consider malformed data within one storage system. A single source normally is maintained by a single instance and can be considered as less messy as a multi source environments. Nevertheless even such system contain malformed data which can not be directly used for analysis.

Schema-less systems can be considered as more error prone than systems enforcing a schema. For comparison see the previous section \ref{sec:unification}. In the former unsatisfied constraints occur whereas in the latter additionally missing attributes and varying entity nesting arise. Those problems are mostly schema related and occur due to poorly designed schemas, technical depts or negligence of specifications. Attribute level specific issues can only partly covered by schema definitions. For instance, missing values are coverable but not spelling errors. Table \ref{tab:singlesource} outlines example conflicts occurring on data unification. 

Missing attributes do not necessarily indicate malformed data. Information entities exist where the absence of data actually provides insights. Declaration on specification level increases the interpretability and analyzability of latent information accordingly. See row one of table \ref{tab:singlesource} for an example.

The second row of table \ref{tab:singlesource} shows an person entity referring to a non-existing address. Data are not usable in the original meaning and have to be re-interpreted. In document-oriented databases information can be nested within sub-entities but basically providing equal information on a different nesting level. Row three in table \ref{tab:singlesource} gives an example. During unification such data structures conflict and require exceptional treatment. 

Another potential conflict arises when it comes to standardizing data with missing values. An example is provided in row four of table \ref{tab:singlesource}. Unset values can be interpreted in two fashions. It can mean on one hand, that the value for that attribute is not captured, i.e. the value exists but is not present. This can be considered as \textit{other}. On the other hand it can imply that it can not be captured, i.e. the attribute is not available for that entity. The interpretation is up to the maintainer of the data source.  A further problem in both cases are missing values on mandatory fields in respect of future data processing.

Misspelling are faulty data from manual entry. The problem of misspellings in heterogeneous data aggregation is the same as in homogeneous environment and therefor plays a very critical role for example when information are displayed to end users.  

\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}[t]{|l|l|}
\hline
Problem & Example \\
\hline
\hline
Missing & Person\textsubscript{1}(name="Fritz", age="23", city="Berlin") \\
attribute & Person\textsubscript{2}(name="Eva", city="Berlin") \\
\hline
Unsatisfied & Person\textsubscript{1}(name="John", addressId="1") \\
constraints & \\
\hline
Varying entity & Person\textsubscript{1}(name="Fritz", street="Spanische Straße 17", city="Berlin") \\
nesting & Person\textsubscript{2}(name"Eva", address=Adress(street="Baumallee 34", city="Berlin")) \\
\hline
Missing & Person\textsubscript{1}(name="Fritz", status="single", city="Berlin") \\
values & Person\textsubscript{2}(name="Eva", status="", city="Berlin") \\
\hline
\end{tabular}
}
\caption{Examples of single source problems}
\label{tab:singlesource}
\end{table}

\subsection{Multi source problems}
All problems from a single source environment can be transfered to a multi source environment. Additionally the absence of commonly valid schema definitions fosters problems when it comes to unification of data and information among various sources. The sub categorization of multi source problems goes along with the sub categorization of single source problems. See right tree in figure \ref{fig:qualityproblems}. On entity level issues due to different degrees of normalization, different formats, naming conflicts or structural conflicts are possible discrepancies. On attribute level different data types, conflicting manifestations, languages and units prevent a simple solution to data unification.

One big challenge in data source fusion is a different degree of normalization or granularity of data. Highly depending on the use case the data are collected for, attributes are broken down into different abstraction levels or not. For that reason one attribute in one source can correspond to two or more attributes in a second source. Depending on the storage technology that is used the same information are de-normalize, i.e. one entity captures all information, or completely normalized, i.e. split up into two entities. Row one in Table \ref{tab:multisource} shows three information entities that basically contain the same information but in different forms of normalization. Normalization of fields requires in some cases significant domain knowledge. A well known problem is the parsing of addresses, especially street names and house numbers as they both contain numeric and alphanumeric characters. From an automation point of view it is impossible to perform accurate splitting of that fields. This is a crucial point if the desired output format requires a higher degree of normalization than some of the underlying sources can provide. 

Row two in table \ref{tab:multisource} shows an example of different schema definition in terms of attribute naming. Fields containing equal information are annotated by a different name. In this example the fields \textit{city} and \textit{town} hold the home town of the person captured. This heterogeneity can also be the source of usage of different languages, e.g. American English and British English. 

Section \ref{sec:unification} provided an overview of different encoding formats. Every one has its on semantic and characteristics. A straight forward aggregation among two data formats is barely possible. For instance, comparing the JSON\cite{json_2017} format with CSV shows, the first supports nesting of entities the other not. XML\cite{xml_2017} and JSON on the other hand support both nesting but distinguish in incompatible data types, as JONS for instance can not represent long integers, whereas XML supports this data type.

Not only technical restrictions but also domain specific restrictions require a different choice of data type for the same type of information. As an example we can consider the ZIP code. For some countries it is sufficient to use an integer type for others its is mandatory to store the postal code as string, as alphanumeric characters are contained. Compare row three in table \ref{tab:multisource}. 

Enums provide a predefined set of manifestations for a certain attribute. Usually designed to avoid messy data enums can in fact cause discrepancies during data unification. This problem is similar to the attribute naming problem mentioned above. Different manifestations stand for the same information value. The problem becomes more severe if a direct mapping among sources into one distinct target format is not possible. For example in system A a entity can accept n different states, whereas in system B m different states are allowed. See row four in table \ref{tab:multisource} for comparison.

Similar to differing manifestations is the problem of different units. Two example entities can be seen in row 5 of table \ref{tab:multisource}. The problem is of a higher complexity as the field quantity is also affected. Especially for mathematical data analysis a common system of units is essential. Combining data from multiple sources must consider such inequalities otherwise aggregations results are faulty.

All multi source problems outlined here can also be applied to schema-less data storages as a single source. 

\begin{table}[htb]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}[t]{|l|l|}
\hline
Problem & Example \\
\hline
\hline
Different degree & Person\textsubscript{1}(name="Fritz", address="Spanische Straße 17, Berlin") \\
of normalization & Person\textsubscript{2}(name="Eva", street="Baumstr.", no="34", city="Berlin") \\
 & Person\textsubscript{3}(name="John", adrId="1"), \\
 & Address(adrId="1", street="Weg", no="2", city="Berlin") \\
\hline
Naming & Person\textsubscript{1}(name="Fritz", city="Berlin") \\
conflict & Person\textsubscript{2}(name="Eva", town="Berlin") \\
\hline
Different & Address\textsubscript{1}(zip="24821", city="Berlin") \\
data types & Address\textsubscript{2}(zip=24821, city="Berlin") \\
\hline
Different & Product\textsubscript{1}(name="apple", status="new") \\
Manifestations & Product\textsubscript{2}(name="apple", status="created") \\
\hline
Different & Product\textsubscript{1}(name="apple", amount="1", unit="ea") \\
Units & Product\textsubscript{2}(name="apple", amount="1", unit="kg") \\
\hline
\end{tabular}
}
\caption{Examples of multi source problems}
\label{tab:multisource}
\end{table}

\section{Assessment of problems for feature extraction\label{sec:problemassesment}}

Based on the previous section the revealed problems are assessed. Indicator like their severity and complexity are considered and finally used to identify concrete issues handled within this work. 
\\\\
Unsatisfied constraints are a general problem in no SQL systems and need to be enforced on application level. Outside the system it is hard to reconstruct malformed constraints. Broken constraints are therefor not considered as a major problem of homogenization of data sources. The same applies to misspelling errors. This type of error is required to run through spell checking software and needs extended manual quality checks for appropriate correction. 
\\\\
The main focus in this work will be on unifying data for automated feature extraction that underly the problems of structural issues. Automation technologies an methods need to be identified and applied reasonable. This covers sub problems like missing attributes, naming issues and varying entity nesting.
\\\\
As second aspect differing levels of normalization are in the focus of this thesis. This problem is considered as significant as mainly information from modern storage systems, e.g. schema-less databases, are to be analyzed. This is due to the fact that they make up a significant amount of latest data in web applications.
\\\\
The third an last main focus lies on problems towards different manifestations. Especially categories and labels for data are very important in modern machine learning approaches. Features extracted from this kind of information provide value in applications like predictions and recommendations.