\chapter{Evaluation of the proposed framework\label{cha:chapter6}}

In this chapter, the concept and the reference implementation of the automated feature extraction pipeline is evaluated. The re-usability and adaptability to different data sources outlined in the previous chapter is summarized and the applicability of the proposed framework assessed. Additionally, we evaluate the quality of the out-coming data, the quality assurance on code level as well as the scalability on throughput basis. 

\section{Applicability of the proposed framework concept}

The most evidence of applicability is given by the reference implementation outlined in detail in the previous chapter. This proof of concept shows that a stepwise approach of data homogenization and a final feature extraction is a solid concept. Providing a generalized library that can be utilized for different semi-structured information entities and in different domains increases the performance up to a high degree of automation in data unification. The framework allows to be extended on demand by further data sources, parsing steps or output adapters as defined in the requirements of chapter \ref{cha:chapter3}.

\section{Performance measurements\label{sec:performance}}

Measuring the performance of the overall implementation is restricted due to the absence of a reference dataset. Depending on the desired quality of outcoming data, which relates to the kind of features finally used, the quality requirements vary. 
\\\\
A more interesting a measurable index is the performance of single processing steps within the parsing pipeline. See figure \ref{fig:basicconcept} as reference. 
\\\\
For the evaluation of the learnability of the system a toy dataset is created and the performance of automated data preparation by using a full-text search engine is evaluated. The toy dataset contains roughly 300 universal ingredients. Those are manually extracted from a dataset of approximately 200 recipes and indexed\footnote{The process of loading documents in to Elasticsearch during which the inverted index is built internally.} in a local Elasticsearch setup. In more concrete terms, 3-grams up to 20-grams for each universal ingredient are used. Raw ingredient strings extracted from the publicly available recipe section of the Kaufland website\footnote{www.kaufland.de/rezepte} are used to verify the accuracy of the suggestions. For evaluation of the performance more than 200 randomly chosen ingredients from Kaufland where manually mapped to the true universal ingredient. 
\\\\
The performance is measured by averaging the index of the true value within the scored Elasticsearch search result. This means the cost of a wrong suggestion increases linearly with the distance to the top scored result. The search result is upper bounded by 15 hits and the cost of a not found element fixed by 16. For instance, when matching the raw ingredient \textit{"Paprika, rot}, the universal ingredient \textit{"rote Paprika"} occurs on index 3 of the result list, 3 is considered as the cost of the miss match. If \textit{"rote Paprika"} is not contained in the result list, 16 is set as the cost. The sum of all costs is averaged across test examples to gain a comparable score. The test dataset is independent from the dataset indexed in elastic search.
\\\\
On the initial run using 200 labeled ingredients and the above descried setup an average score of 1.83 was reached. This means that the average offset from the top of the result list was less than 1. For comparison, if half on the universal ingredient is on index one, and half of the ingredient on index two, an average of 1.5 results. Considering the fact, that the knowledge base of all universal ingredients contains only 200 entries but the cardinality of all possible ingredients is by far higher, this are very promising initial results. 

A second run on an updated Elasticsearch index definition increased the average score by 20\%. Using 2-grams instead of 3-grams as the lower-bound for the ingredient index resulted in an average score of 1.74. This can be explained by the fact, that ingredients with 2 character names in German exists. Those are ignored by an index only using 3 or more grams.

In a third run the toy dataset was extend in a way, such that each of the universal ingredient had exactly one further manifestation. Rerunning the performance evaluation as set up in the second run lead to an average score of 1.67. This number is the perfect prove for the learnability of the proposed feature extraction framework.

\begin{table}[htb]
\centering
\begin{tabular}[t]{|l|l|}
\hline
Setup & Result \\
\hline
\hline
3-grams to 20-grams & 1.83 \\
1 manifestation & \\
\hline
2-grams to 20-grams & 1.74 \\
1 manifestation & \\
\hline
2-grams to 20-grams & 1.67\\ 
2 manifestations & \\
\hline
\end{tabular}
\caption{Performance results of self-learning system}
\label{tab:singlesource}
\end{table}

\section{Test environment\label{sec:testenvir}}

Independent testing of components is a major aspect in software quality. The library components of the proposed framework where unit tested using the official scala-test framework\footnote{www.scalatest.org}. WordSpecs\footnote{www.scalatest.org/at\_a\_glance/WordSpec} provides a very intuitive and human readable setup of tests on code level. Those assure the stability of the library on updates, bug fixes and implementation of further features and will not break a productive system on an update. 
\\\\
The system is very strongly connected to different database systems. The functioning of the different connectors, not only to the underlying data transfer layer also to data sources like MongoDB and the learning system with Elasticsearch must be ensured after maintenance work. 

The independence of the system from actual running instances of the depended databases is very important when it comes to automatized testing. The common approach to achieve the independence is the usage of embedded versions of the required database systems. Open source libraries for an embedded Kafka\footnote{www.github.com/manub/scalatest-embedded-kafka}, embedded Elasticsearch\footnote{www.github.com/allegro/embedded-elasticsearch} and embedded MongoDB\footnote{www.github.com/flapdoodle-oss/de.flapdoodle.embed.mongo} are used for unit and integration tests of the framework implementation.

\section{Scalability\label{sec:scal}}

\subsection{Scalability on domain level}

The extendability of the provided library is a great win for unifying data across several sources. This is mainly possible through the quick extension of input adapters as well as the stepwise approach of parsing data. The introduction of raw entities gives space for input adapters to add more sources, as well as stability to the downstream parsing logic on the other hand. The pre-generalization of data plays a significant role in the flexible approach of the framework and the developed concept of it. Revisiting the analyzed problems from chapter \ref{cha:chapter2} shows, that introduction of the raw entity can be lead back to the categorization of data quality problems. On upstream side, structural discrepancies are handled. On the downstream side, attribute level problems are tackled.
\\\\
A re-usage of the created framework and its concrete implementation in the domain of food products showed that is possible to scale the framework over further domains. This is enabled by the abstraction of data problems on a meta level. The domain specifics can be realized on a higher level of abstraction which does not interfere with the concept of the framework itself.

\subsection{Scalability on data throughput level}

The introduction of this work gives insights of about the novel amount of data available nowadays. The choice of the container framework in section \ref{sec:env} pointed out that in this work no requirement for a technical scalability is defined. Having said this, it is also important to say that the proposed framework and the developed concept does indeed support technical scaling. This is mainly up to the chosen underlying transportation layer. As Kafka is applicable in distributed systems and the framework on top of it is designed such that the data for each information entity to be parsed lives in a different topic, the application can simply be scaled by adding additional parsing nodes by running the same application multiple times.
\\\\
The concept describes the abstraction of the framework implementation of the container framework, as well as the data transpiration layer. If the demand for a distributed data parsing and feature extraction system arises, the proposed framework can be easily ported to distributed frameworks like Apache Flink or Apache Spark. The concrete implementation of the UDFs are simple map functions which can be simply ported to different systems.
